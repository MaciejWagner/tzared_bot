# F2.T4: Inference Engine

## Agent: AI_SENIOR

## Task Overview
Implement the inference engine that runs the neural network and produces actions from screen frames.

## Context
Project: `src/TzarBot.NeuralNetwork/`. Use Microsoft.ML.OnnxRuntime for inference.

## Requirements

1. Create interface:
   ```csharp
   public interface IInferenceEngine : IDisposable
   {
       GameAction Infer(float[] preprocessedInput);
       float[] InferRaw(float[] input);
       bool IsGpuEnabled { get; }
       TimeSpan LastInferenceTime { get; }
   }
   ```

2. Implement `OnnxInferenceEngine`:
   ```csharp
   public class OnnxInferenceEngine : IInferenceEngine
   {
       private readonly InferenceSession _session;
       private readonly IActionDecoder _decoder;

       public OnnxInferenceEngine(byte[] modelData, bool useGpu = false)
       {
           var options = new SessionOptions();
           if (useGpu)
           {
               options.AppendExecutionProvider_CUDA(); // or DirectML
           }
           _session = new InferenceSession(modelData, options);
       }

       public GameAction Infer(float[] input)
       {
           var inputTensor = new DenseTensor<float>(input, GetInputShape());
           var inputs = new List<NamedOnnxValue>
           {
               NamedOnnxValue.CreateFromTensor("input", inputTensor)
           };
           using var results = _session.Run(inputs);
           var output = results.First().AsTensor<float>().ToArray();
           return _decoder.Decode(output);
       }
   }
   ```

3. Create `ActionDecoder`:
   ```csharp
   public interface IActionDecoder
   {
       GameAction Decode(float[] networkOutput);
   }

   public class ActionDecoder : IActionDecoder
   {
       // Network output format:
       // [0:1] - Mouse dx, dy (tanh activated, range [-1, 1])
       // [2:N] - Action probabilities (softmax)
   }
   ```

4. Create `GameAction` in Common:
   ```csharp
   public class GameAction
   {
       public int MouseDeltaX { get; set; }
       public int MouseDeltaY { get; set; }
       public ActionType Type { get; set; }
       public float Confidence { get; set; }
   }

   public enum ActionType
   {
       None, LeftClick, RightClick, DoubleClick,
       DragStart, DragEnd,
       Hotkey1, Hotkey2, ..., Hotkey0,
       CtrlHotkey1, ...,
       ScrollUp, ScrollDown,
       Escape, Enter
   }
   ```

5. Create tests:
   - Test_Inference_ReturnsAction
   - Test_InferenceTime_Under50ms
   - Test_ActionDecoder_CorrectParsing
   - Test_MultipleInferences_NoMemoryLeak

## Outputs
- `src/TzarBot.NeuralNetwork/Inference/IInferenceEngine.cs`
- `src/TzarBot.NeuralNetwork/Inference/OnnxInferenceEngine.cs`
- `src/TzarBot.NeuralNetwork/Inference/ActionDecoder.cs`
- `tests/TzarBot.Tests/Phase2/InferenceTests.cs`

## Test Command
```bash
dotnet test tests/TzarBot.Tests --filter "FullyQualifiedName~Phase2.Inference"
```

## On Failure
If inference slow:
1. Check if GPU provider is working
2. Profile to find bottleneck
3. Consider batching if possible
4. Use DirectML on Windows for GPU
